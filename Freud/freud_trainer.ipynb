{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Freud Mental Health AI - Training Notebook\n",
    "\n",
    "**Goal**: Train a 2.7B parameter model (Phi-2) for empathetic mental health conversations\n",
    "\n",
    "**Hardware**: Kaggle P100 GPU (16GB VRAM)\n",
    "\n",
    "**Method**: QLoRA (Efficient fine-tuning)\n",
    "\n",
    "**Expected Time**: 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Before You Start:\n",
    "\n",
    "1. ‚úÖ Upload `freud_training_data/` folder to Kaggle datasets\n",
    "2. ‚úÖ Enable GPU in Kaggle notebook settings (P100)\n",
    "3. ‚úÖ Enable internet access\n",
    "4. ‚úÖ Have 4+ hours of Kaggle GPU quota available\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "\n",
    "1. Install dependencies\n",
    "2. Load and verify your training data\n",
    "3. Load Phi-2 model with 4-bit quantization\n",
    "4. Configure QLoRA (efficient fine-tuning)\n",
    "5. Train for 3 epochs with checkpoints\n",
    "6. Save and upload to HuggingFace\n",
    "7. Test the model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "This installs all required packages for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages (this takes ~3 minutes)\n",
    "!pip install -q transformers==4.36.2\n",
    "!pip install -q datasets==2.16.1\n",
    "!pip install -q accelerate==0.26.1\n",
    "!pip install -q peft==0.7.1\n",
    "!pip install -q bitsandbytes==0.41.3\n",
    "!pip install -q trl==0.7.10\n",
    "!pip install -q torch==2.1.2\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration\n",
    "\n",
    "All training hyperparameters in one place for easy adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model Settings\n",
    "BASE_MODEL = \"microsoft/phi-2\"  # 2.7B parameter model\n",
    "# Backup option if Phi-2 doesn't fit: \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "# Data Paths (adjust if you uploaded with different name)\n",
    "TRAIN_DATA_PATH = \"/kaggle/input/freud-training-data/train.json\"\n",
    "VAL_DATA_PATH = \"/kaggle/input/freud-training-data/validation.json\"\n",
    "\n",
    "# Output Settings\n",
    "OUTPUT_DIR = \"freud_phi2_model\"\n",
    "HF_MODEL_NAME = \"YourUsername/freud-phi2-mental-health\"  # Change to your HF username\n",
    "\n",
    "# Training Hyperparameters\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4  # Per device batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 16\n",
    "MAX_SEQ_LENGTH = 512\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# QLoRA Settings (for efficient training)\n",
    "LORA_R = 16  # Rank of LoRA matrices\n",
    "LORA_ALPHA = 32  # Scaling factor\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Checkpoint Settings\n",
    "SAVE_STEPS = 500  # Save every 500 steps\n",
    "LOGGING_STEPS = 50  # Log every 50 steps\n",
    "\n",
    "print(\"üìã Configuration loaded:\")\n",
    "print(f\"   Model: {BASE_MODEL}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load and Verify Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ Loading training data...\\n\")\n",
    "\n",
    "# Load datasets\n",
    "with open(TRAIN_DATA_PATH, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(VAL_DATA_PATH, 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_dataset):,}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_dataset):,}\\n\")\n",
    "\n",
    "# Show a sample\n",
    "print(\"üîç Sample Training Example:\\n\")\n",
    "print(\"=\" * 80)\n",
    "sample_text = train_dataset[0]['text']\n",
    "print(sample_text[:500])  # Show first 500 characters\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Model with 4-bit Quantization\n",
    "\n",
    "This loads the Phi-2 model in 4-bit precision to save VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üîÑ Loading {BASE_MODEL} with 4-bit quantization...\\n\")\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üìä Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"üíæ Model size in memory: ~{model.num_parameters() * 0.5 / 1024**3:.1f} GB (4-bit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure QLoRA\n",
    "\n",
    "QLoRA allows us to fine-tune a large model by training only a small adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Configuring QLoRA...\\n\")\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"Wqkv\", \"fc1\", \"fc2\"],  # Phi-2 specific\n",
    "    # For GPT-Neo use: [\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úÖ QLoRA configured!\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"üìä Total parameters: {total_params:,}\")\n",
    "print(f\"\\nüí° We're only training {trainable_params:,} parameters out of {total_params:,}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Setting up training arguments...\\n\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_8bit\",  # 8-bit optimizer to save memory\n",
    "    fp16=True,  # Mixed precision training\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,  # Keep only last 3 checkpoints\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=SAVE_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    \n",
    "    # Other\n",
    "    report_to=\"none\",  # Don't use wandb\n",
    "    push_to_hub=False,  # We'll push manually later\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured!\")\n",
    "print(f\"\\nüìä Training Summary:\")\n",
    "print(f\"   - Total steps: ~{len(train_dataset) * NUM_EPOCHS // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}\")\n",
    "print(f\"   - Checkpoints every: {SAVE_STEPS} steps\")\n",
    "print(f\"   - Total training time: ~3-4 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Trainer and Start Training üöÄ\n",
    "\n",
    "**‚è∞ This will take 3-4 hours. Go grab a coffee!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèãÔ∏è Creating trainer...\\n\")\n",
    "\n",
    "# Create SFT Trainer (Supervised Fine-Tuning)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,  # Don't pack multiple samples together\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer created!\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nüí° This will take ~3-4 hours. The notebook will continue running.\")\n",
    "print(\"üíæ Checkpoints will be saved every 500 steps in case of interruption.\\n\")\n",
    "\n",
    "# Start training\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚è∞ Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nüìä Final Training Loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Saving fine-tuned model...\\n\")\n",
    "\n",
    "# Save the adapter (LoRA weights)\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {OUTPUT_DIR}/\")\n",
    "print(f\"\\nüìÅ Saved files:\")\n",
    "for file in Path(OUTPUT_DIR).glob(\"*\"):\n",
    "    print(f\"   - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Test the Model üß™\n",
    "\n",
    "Let's test if the model generates good responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Testing the fine-tuned model...\\n\")\n",
    "\n",
    "def test_model(user_input: str, emotion: str = \"neutral\"):\n",
    "    \"\"\"Test the model with a user input\"\"\"\n",
    "    \n",
    "    # Create the prompt in training format\n",
    "    prompt = (\n",
    "        \"<|system|>: You are Freud, a calm, empathetic therapeutic AI assistant. \"\n",
    "        \"You respond thoughtfully, kindly, and supportively. \"\n",
    "        \"You ask gentle follow-up questions and never judge the user.\\n\"\n",
    "        f\"<|user|>:\\n\"\n",
    "        f\"[emotion: {emotion}]\\n\"\n",
    "        f\"{user_input}\\n\"\n",
    "        f\"<|assistant|>:\\n\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only assistant's response\n",
    "    if \"<|assistant|>:\" in full_response:\n",
    "        response = full_response.split(\"<|assistant|>:\")[-1].strip()\n",
    "        # Stop at next <|user|> tag if present\n",
    "        if \"<|user|>\" in response:\n",
    "            response = response.split(\"<|user|>\")[0].strip()\n",
    "    else:\n",
    "        response = full_response.strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (\"Hi\", \"greeting\"),\n",
    "    (\"I feel really sad today\", \"sad\"),\n",
    "    (\"I'm so anxious about my exam\", \"anxious\"),\n",
    "    (\"I had a great day!\", \"happy\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for user_input, emotion in test_cases:\n",
    "    print(f\"\\nüë§ User ({emotion}): {user_input}\")\n",
    "    response = test_model(user_input, emotion)\n",
    "    print(f\"ü§ñ Freud: {response}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Merge Adapter and Save Full Model (Optional)\n",
    "\n",
    "This merges the LoRA adapter with the base model for easier deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Merging LoRA adapter with base model...\\n\")\n",
    "\n",
    "# Reload base model (without quantization)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load and merge adapter\n",
    "merged_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "MERGED_OUTPUT_DIR = f\"{OUTPUT_DIR}_merged\"\n",
    "merged_model.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "\n",
    "print(f\"‚úÖ Merged model saved to {MERGED_OUTPUT_DIR}/\")\n",
    "print(\"\\nüí° This is the full model you can deploy directly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Upload to HuggingFace Hub\n",
    "\n",
    "**Important**: Run `!huggingface-cli login` first and enter your HF token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (you'll need to enter your token)\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üì§ Uploading to HuggingFace Hub: {HF_MODEL_NAME}...\\n\")\n",
    "\n",
    "# Push merged model to hub\n",
    "merged_model.push_to_hub(HF_MODEL_NAME, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(HF_MODEL_NAME, use_temp_dir=False)\n",
    "\n",
    "print(f\"‚úÖ Model uploaded successfully!\")\n",
    "print(f\"\\nüîó View your model at: https://huggingface.co/{HF_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "‚úÖ Fine-tuned a 2.7B parameter model for mental health conversations\n",
    "\n",
    "‚úÖ Trained on your ~30K conversation samples\n",
    "\n",
    "‚úÖ Used QLoRA for efficient training\n",
    "\n",
    "‚úÖ Saved checkpoints every 500 steps\n",
    "\n",
    "‚úÖ Tested the model\n",
    "\n",
    "‚úÖ Uploaded to HuggingFace\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download the model** from HuggingFace\n",
    "2. **Update your FastAPI backend** to use the new model\n",
    "3. **Test thoroughly** before deploying to production\n",
    "4. **Monitor performance** and collect feedback\n",
    "\n",
    "---\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "If you encountered any issues, check:\n",
    "- GPU quota remaining (Kaggle gives 30hrs/week)\n",
    "- Dataset path is correct\n",
    "- Model name matches your HuggingFace username\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've successfully trained Freud AI! üß†‚ú®**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
